{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 10\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Connect to the cluster and initiate spark session\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[1;32m      5\u001B[0m spark_session \u001B[38;5;241m=\u001B[39m \u001B[43mSparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuilder\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaster\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark://192.168.2.19:7077\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappName\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest_2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.cores.max\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.ui.showConsoleProgress\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# .config(\"spark.dynamicAllocation.enabled\", True)\\\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# .config(\"spark.shuffle.service.enabled\", True)\\\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# .master(\"spark://192.168.2.19:7077\") \\\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:231\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    228\u001B[0m         sc \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39mgetOrCreate(sparkConf)\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[0;32m--> 231\u001B[0m     session \u001B[38;5;241m=\u001B[39m \u001B[43mSparkSession\u001B[49m\u001B[43m(\u001B[49m\u001B[43msc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    233\u001B[0m     session\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39msessionState()\u001B[38;5;241m.\u001B[39mconf()\u001B[38;5;241m.\u001B[39msetConfString(key, value)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:253\u001B[0m, in \u001B[0;36mSparkSession.__init__\u001B[0;34m(self, sparkContext, jsparkSession)\u001B[0m\n\u001B[1;32m    251\u001B[0m         jsparkSession \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSparkSession\u001B[38;5;241m.\u001B[39mgetDefaultSession()\u001B[38;5;241m.\u001B[39mget()\n\u001B[1;32m    252\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 253\u001B[0m         jsparkSession \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSparkSession(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msc\u001B[49m())\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession \u001B[38;5;241m=\u001B[39m jsparkSession\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39msqlContext()\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "# Connect to the local cluster and initiate spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = SparkSession.builder.appName(\"test_2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import black for code formatting\n",
    "\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data and infer schema automatically from json keys\n",
    "\n",
    "df_raw = spark_session.read.options(multiline=False, header=True).json(\n",
    "    \"sample_data.json\"\n",
    ")\n",
    "df_raw.show(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print to inspect schema\n",
    "\n",
    "df_raw.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 1: Filter out columns that will not be used\n",
    "\n",
    "cols_to_keep = [\"author\", \"body\", \"created_utc\", \"score\", \"subreddit\", \"subreddit_id\"]\n",
    "\n",
    "df_reddit = df_raw.select([col for col in cols_to_keep])\n",
    "df_reddit.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 2.1: Count the biggest subreddits (by number of posts)\n",
    "\n",
    "df_subred_count = df_reddit.groupBy(\"subreddit\").count()\n",
    "df_subred_count.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 2.2: Get list with top 100 subreddit ids\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_count_pd = df_subred_count.toPandas()\n",
    "\n",
    "subs_to_incl = 100\n",
    "\n",
    "df_top_subs = df_count_pd.sort_values(by=\"count\", ascending=False).iloc[0:subs_to_incl]\n",
    "top_subs = df_top_subs[\"subreddit\"].tolist()\n",
    "print(top_subs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 3.1: Filter data to only contain top subreddits\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_sub_filtered = df_reddit.filter(col(\"subreddit\").isin(top_subs))\n",
    "df_sub_filtered.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 4.1: Find out who are active users\n",
    "\n",
    "df_user_count = df_sub_filtered.groupBy(\"author\").count()\n",
    "df_user_count.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 4.2: Find active and inactive users\n",
    "\n",
    "comment_threshold = 1\n",
    "\n",
    "df_top_users = df_user_count.filter(col(\"count\") > comment_threshold)\n",
    "top_users = df_top_users.select(\"author\").rdd.flatMap(lambda x: x).collect()\n",
    "print(top_users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 4.3: Filter out inactive users with few comments\n",
    "\n",
    "df_user_filtered = df_sub_filtered.filter(col(\"author\").isin(top_users))\n",
    "df_user_filtered.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 5.1: Create column with list of subreddits for each user\n",
    "\n",
    "from pyspark.sql.functions import collect_set\n",
    "\n",
    "df_user_subs = df_user_filtered.groupby(\"author\").agg(\n",
    "    collect_set(\"subreddit\").alias(\"subreddit\")\n",
    ")\n",
    "df_user_subs.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 5.2: Create tuples from all those subreddits\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DataType, StructType, StructField\n",
    "\n",
    "tuple_schema = ArrayType(\n",
    "    StructType(\n",
    "        [\n",
    "            StructField(\"tuple_1\", StringType(), False),\n",
    "            StructField(\"tuple_2\", StringType(), False),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def tuple_from_list(lst):\n",
    "    return [(sub_1, sub_2) for sub_1 in lst for sub_2 in lst]\n",
    "\n",
    "\n",
    "tuple_udf = udf(lambda x: tuple_from_list(x), tuple_schema)\n",
    "\n",
    "df_user_subs = df_user_subs.withColumn(\"subreddit_tuples\", tuple_udf(col(\"subreddit\")))\n",
    "df_user_subs.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 5.3: Explode and get the count of each tuple\n",
    "\n",
    "from pyspark.sql.functions import explode, count\n",
    "\n",
    "# Explode the tuples into individual rows\n",
    "df_exploded = df_user_subs.select(explode(\"subreddit_tuples\").alias(\"tuple_col\"))\n",
    "\n",
    "# Group by the exploded tuples and count the occurrences of each tuple\n",
    "df_tuple_counts = exploded_df.groupBy(\"tuple_col\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Show the results\n",
    "df_tuple_counts.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 5.4: Filter out entries where both tuple elements are the same\n",
    "\n",
    "df_counts_filtered = df_tuple_counts.filter(\n",
    "    ~(col(\"tuple_col\").getField(\"tuple_1\") == col(\"tuple_col\").getField(\"tuple_2\"))\n",
    ")\n",
    "df_counts_filtered.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 5.6: Get rid of duplicates\n",
    "\n",
    "result_clean = df_counts_filtered.rdd.map(\n",
    "    lambda row: [(row[0][0], row[0][1]), row[1]]\n",
    ").collect()\n",
    "\n",
    "result_no_dupes = []\n",
    "encountered_pairs = set()\n",
    "\n",
    "for lst in result_clean:\n",
    "    tup = lst[0]\n",
    "    count = lst[1]\n",
    "    sorted_tup = tuple(sorted(tup))\n",
    "\n",
    "    if sorted_tup in encountered_pairs:\n",
    "        continue\n",
    "\n",
    "    encountered_pairs.add(sorted_tup)\n",
    "    result_no_dupes.append([sorted_tup, count])\n",
    "\n",
    "df_result = pd.DataFrame(result_no_dupes, columns=[\"subreddits\", \"count\"]).sort_values(\n",
    "    by=[\"count\"], ascending=False\n",
    ")\n",
    "df_result.head(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}